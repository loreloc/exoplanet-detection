\documentclass[11pt, a4paper]{article}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{appendix}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\date{}
\title{Exoplanets detection using random forest}
\author{Lorenzo Loconte}

\begin{document}

\maketitle
\begin{abstract}
  This work consists of identifying exoplanets using random forest which hyperparameters are automatically optimized with techniques that come from the \texttt{AutoML} research.
  The model is trained using the processed data that comes from the \texttt{NASA Kepler} project of discovering exoplanets (i.e. planets outside our solar system).
  The hyperparameters of the model are optimized and cross-validated with \texttt{Hyperband}, a simple yet effective and scalable method for hyperparameters optimization.
  The dataset used in this work can be found at \textbf{https://exoplanetarchive.ipac.caltech.edu}.
\end{abstract}

\section{Introduction}
  \paragraph{Kepler Object of Interest}
    The dataset used in this work is the cumulative list of the \texttt{Kepler Object of Interest} (\texttt{KOI}) that comes from the \texttt{NASA Kepler} project of discovering exoplanets.
    The dataset is composed by a list of examples regarding exoplanets. Each example have a label indicating if the corresponding exoplanet is candidate, false positive or confirmed. The provided dataset consists of a lot of heterogeneous features. For the classification task the selected features can be found in Appendix \ref{appendix:features}. The candidate exoplanets (i.e. the exoplanets which existence is uncertain) are discarded in order to reduce noise and improve the learning process.
    For simplicity all the examples having null values are removed from the dataset. Furthermore, all the selected features are numeric and so the preprocessing method applied to the dataset is the standard normalization.
    The resulting dataset contains \texttt{6884} samples, which \textasciitilde \texttt{67\%} are false positives and the remaining are confirmed.
    
  \paragraph{Random Forests}
    The model used for the classification task is a random forest, an ensemble of random decision trees. The prediction of the random forest is computed as the mode of each tree's classification. Random decision forests correct the habit of decision trees of overfitting. Each tree of the forest is trained on a subset of the features and samples. The hyperparameters of the random forest are optimized and cross-validated using approaches that come from the \texttt{AutoML} research. The following hyperparameters are optimized:
    \begin{itemize}
      \item The split criterion (Gini impurity or information gain, see Appendix \ref{appendix:splitcriterions} for details)
      \item The percentage of features to use for each tree
      \item The maximum depth of each tree
      \item The minimum number of samples required to split an internal node
      \item The minimum number of samples to be at a leaf node
    \end{itemize}
  As the next section will expose, the number of trees in the random forest is not considered an hyperparameter because it represents the budget of the hyperparameters optimization algorithm chosen. In this work we refer with budget as the computational cost of cross-validating a certain model.

\section{Hyperparameters Tuning}
  In this section it will be shown a formalization of the hyperparameters optimization task as a search problem. In this work we refer to hyperparameters as the ones that cannot be trained (e.g. the ones described in the previous section). After that we briefly introduce the hyperparameters optimizer used in this work, \texttt{Hyperband}, as described in \cite{hyperband}.

  Given a training set $T$, a model $L$ and its hyperparameters search space $\Omega$, the hyperparameters optimization task consists to find a set of hyperparameters $\omega \in \Omega$ such that the resulting model $L(\omega)$ maximizes a certain score $S$ using cross-validation on $T$.

  There are a lot of different techniques for hyperaparameters optimization, some of them are the following:
  \begin{itemize}
    \item Random Search
    \item Grid Search
    \item Hyperband
    \item Bayesian Optimization
    \item Hybrid Approaches (like \texttt{BOHB} \cite{bohb})
  \end{itemize}

  The simplest algorithm is \texttt{Random Search}. It works well if the hyperparameters search space is not too big. \texttt{Random Search} is expensive because each evaluation of the score function is done on models completely built from scratch. So, each evaluation of the score function on some randomly selected hyperparameters will use the same amount of resources, even if some hyperparameters of the search space obtain a very low score. The idea behind \texttt{Hyperband} is that it's better to save up resources on these hyperparamters configurations that doesn't obtain a sufficiently good score by gradually building more complex models over time.
  
  As we said before we refer to budget as the computational cost of cross-validating a certain model. For example, for neural networks the budget can be the number of training epochs, while for random forests it can be the number of trees. We must know that the computational cost of cross-validating a model is directly proportional to the budget.

  Given a budget $B$, \texttt{Hyperband} consists of random sampling some points in the hyperparameters space and cross-validating them using only a fraction of $B$. After that we pick a fraction $1/\eta$ of the best models. In this way we discard the models that don't obtain a sufficiently good score saving up computational resources. After that we strengthen the previous models using a large portion of $B$ and iterate in this way until we use the complete budget $B$. The fraction of the budget to use for each iteration grows geometrically in respect of $\eta$. So, the parameters $B$ and $\eta$ are chosen in such way that we obtain $\eta$ models at the end of the iterations.
  Even if it is a simple algorithm, it is well scalable on multiple CPUs because we assume that every point in the hyperparameters space is independent from the others.

  In our case the model $L$ is a random forest. As said before, the budget is the maximum number of trees that the final random forest will have. The choice of the score $S$ is dependent from the target task. For simplicity we use the $F_{1}$ score (i.e. we want to maximize both \textit{Precision} and \textit{Recall}). In this work the hyperparameters search space can be defined formally as:
  \[\Omega = \overbrace{\{gini, entropy\}}^{\text{Split Criterion}} \times \overbrace{(0, 1]}^{\text{Features fraction}} \times\]
  \[\times \overbrace{\mathbb{N}^{*}}^{\text{Max depth}} \times \overbrace{\mathbb{N}^{*}}^{\text{Min samples to split}} \times \overbrace{\mathbb{N}^{*}}^{\text{Min samples at leaf}}\]
  For obvious reasons only a subset of this space can be explored. Table \ref{table:hyperparameters} shows the subset of the search space used in this work.

  \begin{table}
    \centering
    \begin{tabular}{|c c|}
      \hline
      Hyperparameters & Values \\
      \hline\hline
      Split Criterions & $\{gini, entropy\}$ \\
      \hline
      Features fraction & $(0, 1]$ \\
      \hline
      Maximum Depth & $\{8,9,...,32\}$ \\
      \hline
      Minimum samples to split & $\{2,3,...,16\}$ \\
      \hline
      Minimum samples at a leaf & $\{1,2,...,10\}$ \\
      \hline
    \end{tabular}
    \caption{The hyperparameters space subset used in this work.}
    \label{table:hyperparameters}
  \end{table}

  The result of the hyperparameters optimization task is a point in the hyperparameters space:
  \[\omega=(c, h, k, t, p)\in\Omega\]
  where $c$ is the split criterion, $h$ is the maximum depth, $k$ is the minimum number of samples to split, $t$ is the minimum number of samples at a leaf, $p$ is the features percentage for each tree.

\section{Conclusion and Results}
  As you can see from Table \ref{table:benchmark} the auto-tuned random forest used in this work obtained a way better \textit{Precision} and \textit{Recall}. We can choose any score function for hyperparameters optimization. For example we can decide to maximize only one between \textit{Precision} and \textit{Recall} scores. Furthermore, it's possible to use other metrics like \textit{Average Precision}.
  
  As you can see from Table \ref{table:confusion} the false positives count (the number of examples, which are predicted as exoplanets, but that are not) and the false negatives count (the number of actual exoplanets not being discovered), are pretty low.

  In the end we present Table \ref{table:importances} that shows the importances of the first five out eighteen features used.
  
  \begin{table}
    \centering
    \begin{tabular}{|c c c c|}
      \hline
      Model & Precision & Recall & $F_{1}$  \\
      \hline\hline
      k-NN & 0.922 & 0.829 & 0.873 \\
      \hline
      SVC & 0.925 & 0.875 & 0.899 \\
      \hline
      2-layer NN & 0.930 & 0.917 & 0.926 \\
      \hline
      Random Forest & 0.938 & 0.932 & 0.938 \\
      \hline
      \textbf{Random Forest w/\texttt{Hyperband}} & \textbf{0.949} & \textbf{0.937} & \textbf{0.943} \\
      \hline
    \end{tabular}
    \caption{\textit{Precision}, \textit{Recall} and $F_{1}$ metrics of several models for comparison. The models hyperparameters are the default ones implemented in \texttt{sklearn}. For \texttt{Hyperband} we used $B=144$ and $\eta=3$.}
    \label{table:benchmark}
  \end{table}
  
  \begin{table}[b]
    \centering
    \begin{tikzpicture}[scale=1.67]
      % Draw the box
      \draw[thick] (0, 0) -- (2, 0);
      \draw[thick] (0, 0) -- (0, 2);
      \draw[thick] (2, 2) -- (2, 0);
      \draw[thick] (2, 2) -- (0, 2);
      \draw[thick] (0, 1) -- (2, 1);
      \draw[thick] (1, 0) -- (1, 2);
      % Draw the headers
      \node[rotate=0] (h1) at (1, 2.5) {\textbf{Actual}};
        \node[rotate=0] (h11) at (0.5, 2.25) {Positive};
        \node[rotate=0] (h12) at (1.5, 2.25) {Negative};
      \node[rotate=90] (h2) at (-0.5, 1) {\textbf{Predicted}};
        \node[rotate=90] (h21) at (-0.25, 1.5) {Positive};
        \node[rotate=90] (h22) at (-0.25, 0.5) {Negative};
      % Draw the category values
      \coordinate[label={\textbf{414}}] (PP) at (0.5, 1.33);
      \coordinate[label={ 58}] (NP) at (0.5, 0.33);
      \coordinate[label={ 46}] (PN) at (1.5, 1.33);
      \coordinate[label={\textbf{859}}] (NN) at (1.5, 0.33);
    \end{tikzpicture}
    \caption{Confusion matrix over the test set.}
    \label{table:confusion}
  \end{table}

  \begin{table}
    \centering
    \begin{tabular}{|c c c|}
    \hline
    Feature \# & Description & Importance \\
    \hline\hline
    7 & Planetary Radius & 0.220 \\
    5 & Planet-Star Radius Ratio & 0.107 \\
    1 & Orbital Period & 0.093 \\
    6 & Fitted Stellar Density & 0.089 \\
    8 & Orbit Semi-Major Axis & 0.065 \\
    \hline
    \end{tabular}
    \caption{Features importances in descending order.}
    \label{table:importances}
  \end{table}

\vspace*{1.25cm}
\appendix
\appendixpage
\section{Decision tree split criterions}
  \label{appendix:splitcriterions}
  \subsection{Gini impurity}
    Citing \texttt{wikipedia.org}, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. To compute Gini impurity for a set of items with $J$ classes, suppose $i\in\{1,2,...,J\}$, and let $p_{i}$ be the faction of items labeled with class $i$ in the set.
    \[{I} _{G}(p)=\sum _{i=1}^{J}p_{i}\sum _{k\neq i}p_{k}=\sum _{i=1}^{J}p_{i}(1-p_{i})=\sum _{i=1}^{J}p_{i}-\sum _{i=1}^{J}{p_{i}}^{2}=1-\sum _{i=1}^{J}{p_{i}}^{2}\]
  \subsection{Information gain}
    Information gain is based on the concept of entropy and information content from information theory. For each node of the tree, the information value represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given the example reached that node. Given $T$ the set of training examples, suppose $i\in\{1,2,...,J\}$, where $J$ is the number of classes, and let $p_{i}$ be the faction of items labeled with class $i$ in the training set.
    \[IG(T, a)=\mathrm{H}(T)-\mathrm{H}(T|a)=-\sum _{i=1}^{J}p_{i}\log _{2}{p_{i}}-\sum _{a}{p(a)\sum _{i=1}^{J}-p(i|a)\log _{2}{p(i|a)}}\]

\section{Exoplanet Features}
  \label{appendix:features}
  \setlength{\columnsep}{0mm}
  \begin{multicols}{2}
    \begin{enumerate}
      \item Orbital Period [$days$]
      \item Impact Parameter
      \item Transit Duration [$hrs$]
      \item Transit Depth [$ppm$]
      \item Planet-Star Radius Ratio
      \item Fitted Stellar Density [$g/cm^{3}$]
      \item Planetary Radius [$Earth\:radii$]
      \item Orbit Semi-Major Axis [$AU$]
      \item Inclination [$deg$]
      \item Equilibrium Temperature [$K$]
      \item Insolation Flux [$Earth\:flux$]
      \item Stellar Effective Temperature [$K$]
      \item Stellar Surface Gravity [$\log_{10}(cm/s^{2})$]
      \item Stellar Radius [$Solar\:radii$]
      \item Stellar Mass [$Solar\:mass$]
      \item RA [$decimal\:degrees$]
      \item Dec [$decimal\:degrees$]
      \item Kepler-band [$mag$]
    \end{enumerate}
  \end{multicols}

\bibliography{bibliography.bib}
\bibliographystyle{unsrt}

\end{document}
